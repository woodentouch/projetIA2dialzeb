# üêç Snake RL - Reinforcement Learning Comparison

Projet de comparaison d'algorithmes de Reinforcement Learning (PPO, DQN, SAC) appliqu√©s au jeu Snake.

## üìã Description

Ce projet impl√©mente et compare trois algorithmes de Deep Reinforcement Learning pour apprendre √† un agent √† jouer au jeu Snake :

- **PPO (Proximal Policy Optimization)** - Algorithme on-policy stable et robuste
- **DQN (Deep Q-Network)** - Algorithme off-policy classique avec replay buffer
- **SAC (Soft Actor-Critic)** - Algorithme off-policy moderne avec optimisation d'entropie

## üéØ Objectifs

- Impl√©menter un environnement Snake compatible avec Gymnasium
- Entra√Æner et comparer les performances de PPO, DQN et SAC
- Analyser les forces et faiblesses de chaque algorithme
- Visualiser les agents entra√Æn√©s


### Setup

```bash
# Cloner le repository
git clone https://github.com/Unity1202/Snake_RL.git
cd snake_RL_Alexis_Clement_Gregoire

# Cr√©er un environnement virtuel
python3 -m venv venv
source venv/bin/activate  # Sur Windows: venv\Scripts\activate

# Installer les d√©pendances
pip install -r requirements.txt
```

## üéÆ Utilisation

### 1. Entra√Æner les agents

```bash
# Entra√Æner PPO (recommand√© en premier)
cd training
python train_ppo.py

# Entra√Æner DQN
python train_dqn.py

# Entra√Æner SAC
python train_sac.py
```

**Dur√©e d'entra√Ænement :** ~1-2 heures par algorithme (3M steps)

### 2. Suivre l'entra√Ænement avec TensorBoard

```bash
# Dans un nouveau terminal
tensorboard --logdir=logs
```

Ouvrir http://localhost:6006 dans votre navigateur

### 3. Visualiser les agents entra√Æn√©s

```bash
# Mode interactif
python play_agent.py

# Ou avec arguments
python play_agent.py --algo ppo --episodes 5 --speed 2 --grid-size 6
python play_agent.py --algo ppo --checkpoint 100000 --grid-size 6

# √âvaluation rapide
python play_agent.py --algo ppo --episodes 20 --eval-only

# Visualiser la progression
python play_agent.py --algo ppo --progression --progression-start 10000 --progression-end 100000 --progression-interval 10000
```

#### Options disponibles

| Option | Description | Exemple |
|--------|-------------|---------|
| `--algo` | Algorithme (ppo/dqn/sac) | `--algo ppo` |
| `--episodes` | Nombre d'√©pisodes | `--episodes 10` |
| `--grid-size` | Taille de la grille (d√©faut: 10) | `--grid-size 6` |
| `--speed` | Vitesse (1=normal, 0=max) | `--speed 5` |
| `--model` | Chemin vers le mod√®le | `--model training/models/ppo/best_model.zip` |
| `--checkpoint` | Nombre de steps du checkpoint √† charger | `--checkpoint 100000` |
| `--list-checkpoints` | Lister tous les checkpoints disponibles | `--list-checkpoints` |
| `--no-render` | Pas de rendu visuel | `--no-render` |
| `--eval-only` | Stats uniquement | `--eval-only` |
| `--progression` | Visualiser la progression en testant les checkpoints | `--progression` |
| `--progression-start` | Checkpoint de d√©part pour --progression | `--progression-start 10000` |
| `--progression-end` | Checkpoint de fin pour --progression | `--progression-end 100000` |
| `--progression-interval` | Intervalle entre checkpoints | `--progression-interval 10000` |

## üèóÔ∏è Architecture

### Environnement Snake

**Observation Space:**
- Grille 3D : `(grid_size, grid_size, 3)` (d√©faut: 6x6 pour entra√Ænement, 10x10 pour visualisation)
  - Channel 0 : Position du serpent (1=corps, 2=t√™te)
  - Channel 1 : Position de la nourriture
  - Channel 2 : Murs/limites

**Action Space:**
- Discrete(4) : Haut, Bas, Gauche, Droite

**Syst√®me de r√©compenses:**
- +10 + (longueur √ó 0.5) : Manger la nourriture
- +0.3 √ó (am√©lioration distance) : Se rapprocher de la nourriture
- -0.4 √ó (d√©gradation distance) : S'√©loigner de la nourriture
- -10 : Collision (mur ou auto-collision)
- -0.01 : P√©nalit√© par step
- -0.3 √ó (nombre de boucles) : D√©tection de boucles r√©p√©titives
- +0.05 / -0.2 : Bonus/malus selon l'espace libre disponible
- +100 : Victoire (grille remplie)
- P√©nalit√© de faim croissante (quadratique avec le temps)

### Hyperparam√®tres

#### PPO
```python
learning_rate = 3e-4
n_steps = 2048
batch_size = 64
n_epochs = 10
gamma = 0.99
gae_lambda = 0.95
clip_range = 0.2
ent_coef = 0.01
vf_coef = 0.5
max_grad_norm = 0.5
```

#### DQN
```python
learning_rate = 1e-4
buffer_size = 100,000
learning_starts = 10,000
batch_size = 32
tau = 1.0
gamma = 0.99
train_freq = 4
gradient_steps = 1
target_update_interval = 1000
exploration_fraction = 0.3
exploration_initial_eps = 1.0
exploration_final_eps = 0.05
```

#### SAC
```python
learning_rate = 3e-4
buffer_size = 50,000
learning_starts = 5,000
batch_size = 128
tau = 0.005
gamma = 0.99
train_freq = 4
gradient_steps = 1
ent_coef = "auto"
target_update_interval = 1
target_entropy = "auto"
```

**Note :** SAC est moins adapt√© aux actions discr√®tes comme Snake

## üõ†Ô∏è Technologies Utilis√©es

- **Gymnasium** - Framework d'environnements RL
- **Stable-Baselines3** - Impl√©mentations des algorithmes RL
- **PyTorch** - Backend pour les r√©seaux de neurones
- **Pygame** - Rendu visuel du jeu
- **TensorBoard** - Visualisation de l'entra√Ænement
- **NumPy** - Calculs num√©riques
- **tqdm** - Barres de progression
- **rich** - Affichage format√© dans le terminal

## üìà M√©triques de Comparaison

- **Score moyen** : Longueur du serpent atteinte
- **Temps d'entra√Ænement** : Dur√©e pour 500k steps
- **Stabilit√©** : Variance des performances
- **Sample efficiency** : Nombre d'exp√©riences n√©cessaires
- **Comportement** : Qualit√© des strat√©gies apprises

## üêõ Probl√®mes Connus

- **Boucles locales** : L'agent peut parfois tourner autour de la nourriture (p√©nalit√© de boucles impl√©ment√©e)
- **SAC moins adapt√©** : Con√ßu pour actions continues, n√©cessite un wrapper pour actions discr√®tes ‚Üí Optimis√© avec `train_freq=4` et `batch_size=128`

## üî¨ Am√©liorations Futures

- [ ] Ajouter A2C et PPO avec LSTM
- [ ] Impl√©menter curriculum learning
- [ ] Tester sur grilles plus grandes (15x15, 20x20)
- [ ] Ajouter des obstacles
- [ ] Multi-agent Snake

## üìö R√©f√©rences

- [Stable-Baselines3 Documentation](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DQN Paper](https://arxiv.org/abs/1312.5602)
- [SAC Paper](https://arxiv.org/abs/1801.01290)

