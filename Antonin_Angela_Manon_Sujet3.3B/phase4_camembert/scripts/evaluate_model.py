#!/usr/bin/env python3
"""
Script d'√©valuation du mod√®le CamemBERT sur le test set

Usage:
    python evaluate_model.py
"""

import sys
from pathlib import Path
import torch
from torch.utils.data import DataLoader
import pandas as pd
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Ajouter le dossier src au path (depuis phase4_camembert/scripts)
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

from models.camembert_multitask import CamemBERTMultitask
from models.config import EMOTION_LABELS, SENTIMENT_LABELS, IRONY_LABELS
from training.train import MultiTaskDataset
from transformers import CamembertTokenizer


def load_model(checkpoint_path: str, device: str):
    """
    Charge le mod√®le depuis un checkpoint
    
    Args:
        checkpoint_path: Chemin vers le checkpoint
        device: Device
    
    Returns:
        Mod√®le charg√©
    """
    print(f"üì• Chargement du mod√®le depuis {checkpoint_path}...")
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    
    model_config = checkpoint['model_config']
    model = CamemBERTMultitask(model_config)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)
    model.eval()
    
    print("‚úÖ Mod√®le charg√© avec succ√®s")
    return model


def evaluate_on_test(model, test_loader, device):
    """
    √âvalue le mod√®le sur le test set
    
    Args:
        model: Mod√®le
        test_loader: DataLoader de test
        device: Device
    
    Returns:
        Dictionnaire avec pr√©dictions et labels
    """
    print("\nüß™ √âvaluation sur le test set...")
    
    model.eval()
    
    all_emotion_preds = []
    all_emotion_labels = []
    all_sentiment_preds = []
    all_sentiment_labels = []
    all_irony_preds = []
    all_irony_labels = []
    
    with torch.no_grad():
        for batch in test_loader:
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            
            emotion_preds, sentiment_preds, irony_preds = model.predict(
                input_ids=input_ids,
                attention_mask=attention_mask
            )
            
            all_emotion_preds.extend(emotion_preds.cpu().numpy())
            all_emotion_labels.extend(batch['emotion_label'].numpy())
            all_sentiment_preds.extend(sentiment_preds.cpu().numpy())
            all_sentiment_labels.extend(batch['sentiment_label'].numpy())
            all_irony_preds.extend(irony_preds.cpu().numpy())
            all_irony_labels.extend(batch['irony_label'].numpy())
    
    return {
        'emotion': {
            'preds': np.array(all_emotion_preds),
            'labels': np.array(all_emotion_labels)
        },
        'sentiment': {
            'preds': np.array(all_sentiment_preds),
            'labels': np.array(all_sentiment_labels)
        },
        'irony': {
            'preds': np.array(all_irony_preds),
            'labels': np.array(all_irony_labels)
        }
    }


def plot_confusion_matrices(results, save_dir="results"):
    """
    G√©n√®re les matrices de confusion pour les 3 t√¢ches
    
    Args:
        results: R√©sultats de l'√©valuation
        save_dir: Dossier de sauvegarde
    """
    save_dir = Path(save_dir)
    save_dir.mkdir(exist_ok=True)
    
    fig, axes = plt.subplots(1, 3, figsize=(18, 5))
    fig.suptitle('Matrices de Confusion - Test Set', fontsize=16)
    
    # √âmotion
    cm_emotion = confusion_matrix(results['emotion']['labels'], results['emotion']['preds'])
    sns.heatmap(cm_emotion, annot=True, fmt='d', ax=axes[0], cmap='Blues',
                xticklabels=list(EMOTION_LABELS.values()),
                yticklabels=list(EMOTION_LABELS.values()))
    axes[0].set_title('√âmotions (7 classes)')
    axes[0].set_ylabel('Vrai')
    axes[0].set_xlabel('Pr√©dit')
    
    # Sentiment
    cm_sentiment = confusion_matrix(results['sentiment']['labels'], results['sentiment']['preds'])
    sns.heatmap(cm_sentiment, annot=True, fmt='d', ax=axes[1], cmap='Greens',
                xticklabels=list(SENTIMENT_LABELS.values()),
                yticklabels=list(SENTIMENT_LABELS.values()))
    axes[1].set_title('Sentiment (3 classes)')
    axes[1].set_ylabel('Vrai')
    axes[1].set_xlabel('Pr√©dit')
    
    # Ironie
    cm_irony = confusion_matrix(results['irony']['labels'], results['irony']['preds'])
    sns.heatmap(cm_irony, annot=True, fmt='d', ax=axes[2], cmap='Oranges',
                xticklabels=list(IRONY_LABELS.values()),
                yticklabels=list(IRONY_LABELS.values()))
    axes[2].set_title('Ironie (2 classes)')
    axes[2].set_ylabel('Vrai')
    axes[2].set_xlabel('Pr√©dit')
    
    plt.tight_layout()
    
    save_path = save_dir / "confusion_matrices.png"
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    print(f"\nüìä Matrices de confusion sauvegard√©es: {save_path}")
    plt.show()


def print_classification_reports(results):
    """
    Affiche les rapports de classification pour les 3 t√¢ches
    
    Args:
        results: R√©sultats de l'√©valuation
    """
    print("\n" + "=" * 80)
    print("üìã RAPPORTS DE CLASSIFICATION")
    print("=" * 80)
    
    # √âmotion
    print("\nüé≠ √âMOTIONS:")
    print(classification_report(
        results['emotion']['labels'],
        results['emotion']['preds'],
        target_names=list(EMOTION_LABELS.values()),
        digits=4
    ))
    
    # Sentiment
    print("\nüí≠ SENTIMENT:")
    print(classification_report(
        results['sentiment']['labels'],
        results['sentiment']['preds'],
        target_names=list(SENTIMENT_LABELS.values()),
        digits=4
    ))
    
    # Ironie
    print("\nüòè IRONIE:")
    print(classification_report(
        results['irony']['labels'],
        results['irony']['preds'],
        target_names=list(IRONY_LABELS.values()),
        digits=4
    ))


def main():
    """Fonction principale"""
    
    print("=" * 80)
    print("üéØ √âVALUATION DU MOD√àLE CAMEMBERT")
    print("=" * 80)
    
    # Device
    device = "cuda" if torch.cuda.is_available() else "mps" if torch.backends.mps.is_available() else "cpu"
    print(f"\nüñ•Ô∏è  Device: {device}")
    
    # Charger le mod√®le (depuis phase4_camembert/scripts)
    checkpoint_path = "../../models/best_model.pt"
    model = load_model(checkpoint_path, device)
    
    # Charger les donn√©es de test (depuis phase4_camembert/scripts)
    print("\nüìÇ Chargement des donn√©es de test...")
    test_df = pd.read_csv("../../data/processed/test.csv")
    print(f"   ‚úì Test: {len(test_df)} exemples")
    
    # Tokenizer
    tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
    
    # Dataset et DataLoader
    test_dataset = MultiTaskDataset(
        texts=test_df['text'].tolist(),
        emotion_labels=test_df['emotion_id'].tolist(),
        sentiment_labels=test_df['sentiment'].tolist(),
        irony_labels=test_df['is_ironic'].tolist(),
        tokenizer=tokenizer,
        max_length=128
    )
    
    test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False)
    
    # √âvaluation
    results = evaluate_on_test(model, test_loader, device)
    
    # Afficher les rapports
    print_classification_reports(results)
    
    # G√©n√©rer les matrices de confusion
    plot_confusion_matrices(results)
    
    print("\n‚úÖ √âvaluation termin√©e !")


if __name__ == "__main__":
    main()
