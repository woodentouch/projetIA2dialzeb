import torch
from transformers import pipeline
import gc
from app.config import settings

# Variables globales pour conserver le mod√®le en m√©moire (Cache)
# Cela √©vite de recharger le mod√®le √† chaque phrase si c'est le m√™me mod√®le
current_model_id = None
pipe = None

def get_prediction(text: str, model_type: str):
    """
    Charge le mod√®le demand√© s'il n'est pas d√©j√† actif, 
    lib√®re la m√©moire GPU si n√©cessaire, et effectue l'analyse.
    """
    global current_model_id, pipe
    
    # 1. R√©cup√©ration du chemin Hugging Face via le dictionnaire dans config.py
    model_repo = settings.MODELS.get(model_type.lower())
    
    if not model_repo:
        raise ValueError(f"Le mod√®le '{model_type}' n'est pas configur√© dans settings.MODELS")

    # 2. Gestion du changement de mod√®le pour √©conomiser la VRAM (4Go de ta 3050 Ti)
    if current_model_id != model_repo:
        print(f"üîÑ Changement de mod√®le d√©tect√©...")
        print(f"üì• Chargement de : {model_repo}")
        
        # On d√©truit l'ancien pipeline s'il existe
        pipe = None
        
        # Nettoyage forc√© de la m√©moire RAM et VRAM
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print("üßπ VRAM lib√©r√©e avec succ√®s.")
            
        # Chargement du nouveau pipeline sur le p√©riph√©rique d√©fini (GPU ou CPU)
        try:
            pipe = pipeline(
                "text-classification", 
                model=model_repo, 
                device=settings.DEVICE  # Utilise 0 pour GPU, -1 pour CPU
            )
            current_model_id = model_repo
            print(f"‚úÖ Mod√®le {model_type.upper()} pr√™t sur {'GPU' if settings.DEVICE == 0 else 'CPU'}")
        except Exception as e:
            print(f"‚ùå Erreur lors du chargement du mod√®le : {e}")
            raise

    # 3. Ex√©cution de l'inf√©rence
    # On limite le texte √† 512 tokens pour respecter la limite native de BERT/RoBERTa
    try:
        # L'analyse est effectu√©e ici
        results = pipe(text[:512])
        
        # Le r√©sultat est une liste, on prend le premier √©l√©ment
        # Format attendu : {'label': 'LABEL_X', 'score': 0.99}
        prediction = results[0]
        
        print(f"üîç Analyse termin√©e : {prediction['label']} (Confiance: {prediction['score']:.2%})")
        return prediction

    except Exception as e:
        print(f"‚ùå Erreur lors de l'inf√©rence : {e}")
        raise