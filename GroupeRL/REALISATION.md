# RÃ©alisation du Projet RL : ContrÃ´le & Jeux

## ğŸ“‹ Vue d'ensemble du projet

**Objectif** : Apprendre Ã  un agent Ã  jouer Ã  un jeu vidÃ©o ou contrÃ´ler un systÃ¨me physique en utilisant le Reinforcement Learning.

**Librairies principales** :
- **Stable-Baselines3** : ImplÃ©mentations modernes des algorithmes RL (PPO, DQN, SAC)
- **Gymnasium** : Environnements standardisÃ©s pour tester les agents RL

**Algorithmes Ã  comparer** : PPO, DQN, SAC

---

## ğŸ¯ Ã‰tapes de rÃ©alisation

### Ã‰tape 1 : Configuration de l'environnement

#### 1.1 CrÃ©er un environnement virtuel Python
```bash
python -m venv venv
# Sur Windows
venv\Scripts\activate
```

#### 1.2 Installer les dÃ©pendances requises
```bash
pip install stable-baselines3 gymnasium pygame numpy matplotlib tensorboard
```

**Explication des packages** :
- `stable-baselines3` : Les algos PPO, DQN, SAC
- `gymnasium` : Les environnements de jeu
- `pygame` : Pour visualiser les jeux
- `numpy` : Calculs numÃ©riques
- `matplotlib` : Visualisation des rÃ©sultats
- `tensorboard` : Suivi de l'entraÃ®nement

---

### Ã‰tape 2 : Choisir l'environnement de test

#### Deux catÃ©gories possibles :

**Option A : Jeux vidÃ©o (recommandÃ© pour dÃ©marrer)**
- `CartPole-v1` â­ (PLUS SIMPLE - Commencer ici)
- `LunarLander-v2` (Niveau moyen)
- `Breakout-v4` (Niveau avancÃ© - nÃ©cessite `stable-baselines3[atari]`)

**Option B : SystÃ¨mes physiques (plus complexes)**
- `Pendulum-v1` (Pendule inversÃ©)
- `MountainCar-v0` (Voiture en montagne)

### âœ… Recommandation pour dÃ©buter :
**Commencer avec `CartPole-v1`** (simple, rapide, bon pour les tests)

---

### Ã‰tape 3 : CrÃ©er les scripts de base

CrÃ©er la structure suivante dans le dossier du projet :

```
GroupeRL/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â”œâ”€â”€ train_dqn.py
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â”œâ”€â”€ test_agent.py
â”‚   â””â”€â”€ benchmark_algos.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ ppo_cartpole.zip
â”‚   â”œâ”€â”€ dqn_cartpole.zip
â”‚   â””â”€â”€ sac_cartpole.zip
â”œâ”€â”€ results/
â”‚   â””â”€â”€ comparaison_algos.png
â””â”€â”€ REALISATION.md
```

---

### Ã‰tape 4 : EntraÃ®ner les trois algorithmes

#### 4.1 Script d'entraÃ®nement PPO

**Fichier** : `scripts/train_ppo.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import EvalCallback

# CrÃ©er l'environnement
env = gym.make("CartPole-v1")

# CrÃ©er le modÃ¨le PPO
model = PPO(
    "MlpPolicy",
    env,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    learning_rate=3e-4,
    verbose=1,
    device="cpu"  # ou "cuda" si GPU disponible
)

# EntraÃ®ner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/ppo_cartpole")

env.close()
print("âœ… EntraÃ®nement PPO terminÃ© !")
```

#### 4.2 Script d'entraÃ®nement DQN

**Fichier** : `scripts/train_dqn.py`

```python
import gymnasium as gym
from stable_baselines3 import DQN

# CrÃ©er l'environnement
env = gym.make("CartPole-v1")

# CrÃ©er le modÃ¨le DQN
model = DQN(
    "MlpPolicy",
    env,
    learning_rate=1e-3,
    buffer_size=10000,
    learning_starts=1000,
    target_update_interval=500,
    verbose=1,
    device="cpu"
)

# EntraÃ®ner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/dqn_cartpole")

env.close()
print("âœ… EntraÃ®nement DQN terminÃ© !")
```

#### 4.3 Script d'entraÃ®nement SAC

**Fichier** : `scripts/train_sac.py`

```python
import gymnasium as gym
from stable_baselines3 import SAC

# CrÃ©er l'environnement
env = gym.make("CartPole-v1")

# CrÃ©er le modÃ¨le SAC
model = SAC(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    buffer_size=10000,
    learning_starts=100,
    verbose=1,
    device="cpu"
)

# EntraÃ®ner
model.learn(total_timesteps=50000)

# Sauvegarder
model.save("models/sac_cartpole")

env.close()
print("âœ… EntraÃ®nement SAC terminÃ© !")
```

**Ã€ FAIRE DANS CETTE Ã‰TAPE** :
1. CrÃ©er les fichiers `train_ppo.py`, `train_dqn.py`, `train_sac.py`
2. ExÃ©cuter chaque script :
   ```bash
   python scripts/train_ppo.py
   python scripts/train_dqn.py
   python scripts/train_sac.py
   ```
3. Attendre que les 3 entraÃ®nements se terminent (â±ï¸ 5-10 minutes au total)

---

### Ã‰tape 5 : Tester les agents entraÃ®nÃ©s

#### 5.1 Script de test simple

**Fichier** : `scripts/test_agent.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO, DQN, SAC

# CrÃ©er l'environnement
env = gym.make("CartPole-v1", render_mode="human")

# Tester chaque modÃ¨le
models = {
    "PPO": PPO.load("models/ppo_cartpole", env=env),
    "DQN": DQN.load("models/dqn_cartpole", env=env),
    "SAC": SAC.load("models/sac_cartpole", env=env)
}

for algo_name, model in models.items():
    print(f"\nğŸ® Test de {algo_name}...")
    
    # 5 Ã©pisodes de test
    for episode in range(5):
        obs, info = env.reset()
        done = False
        total_reward = 0
        steps = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, info = env.step(action)
            done = terminated or truncated
            total_reward += reward
            steps += 1
        
        print(f"  Episode {episode+1}: Score = {total_reward:.0f}, Ã‰tapes = {steps}")
    
    print(f"âœ… Tests de {algo_name} terminÃ©s !")

env.close()
```

**Ã€ FAIRE** :
```bash
python scripts/test_agent.py
```

Vous verrez une **fenÃªtre de jeu** s'afficher avec le bÃ¢ton qui essaie de rester Ã©quilibrÃ©. Les agents entraÃ®nÃ©s vont contrÃ´ler le mouvement du chariot.

---

### Ã‰tape 6 : Comparer les performances

#### 6.1 Benchmark des trois algorithmes

**Fichier** : `scripts/benchmark_algos.py`

```python
import gymnasium as gym
import numpy as np
import matplotlib.pyplot as plt
from stable_baselines3 import PPO, DQN, SAC

def evaluate_agent(model, env, num_episodes=10):
    """Ã‰value un agent sur plusieurs Ã©pisodes"""
    scores = []
    
    for _ in range(num_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        
        while not done:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, terminated, truncated, _ = env.step(action)
            done = terminated or truncated
            episode_reward += reward
        
        scores.append(episode_reward)
    
    return scores

# CrÃ©er l'environnement
env = gym.make("CartPole-v1")

# Charger les modÃ¨les
models = {
    "PPO": PPO.load("models/ppo_cartpole"),
    "DQN": DQN.load("models/dqn_cartpole"),
    "SAC": SAC.load("models/sac_cartpole")
}

# Ã‰valuer tous les modÃ¨les
results = {}
for algo_name, model in models.items():
    print(f"Ã‰valuation de {algo_name}...")
    scores = evaluate_agent(model, env, num_episodes=20)
    results[algo_name] = scores
    print(f"  Moyenne: {np.mean(scores):.2f}")
    print(f"  Ã‰cart-type: {np.std(scores):.2f}")
    print(f"  Min/Max: {np.min(scores):.0f}/{np.max(scores):.0f}")

# Afficher les rÃ©sultats
fig, axes = plt.subplots(1, 2, figsize=(14, 5))

# Graphique 1 : BoÃ®tes Ã  moustaches
axes[0].boxplot([results[algo] for algo in results.keys()], 
                labels=list(results.keys()))
axes[0].set_ylabel("Score")
axes[0].set_title("Comparaison des scores (CartPole-v1)")
axes[0].grid(True, alpha=0.3)

# Graphique 2 : Moyenne et Ã©cart-type
means = [np.mean(results[algo]) for algo in results.keys()]
stds = [np.std(results[algo]) for algo in results.keys()]
x = np.arange(len(results))
axes[1].bar(x, means, yerr=stds, capsize=10, alpha=0.7)
axes[1].set_xticks(x)
axes[1].set_xticklabels(list(results.keys()))
axes[1].set_ylabel("Score moyen")
axes[1].set_title("Score moyen avec intervalle de confiance")
axes[1].grid(True, alpha=0.3, axis='y')

plt.tight_layout()
plt.savefig("results/comparaison_algos.png", dpi=100)
print("\nğŸ“Š Graphique sauvegardÃ© dans 'results/comparaison_algos.png'")
plt.show()

env.close()
```

**Ã€ FAIRE** :
```bash
python scripts/benchmark_algos.py
```

Cela gÃ©nÃ©rera un graphique comparant les trois algorithmes.

---

## ğŸ® Guide complet pour tester avec des jeux

### Option 1 : Visualisation simple pendant l'entraÃ®nement

**Ajouter ceci Ã  votre script d'entraÃ®nement** :

```python
import gymnasium as gym
from stable_baselines3 import PPO

# Avec render_mode="human" pour afficher le jeu
env = gym.make("CartPole-v1", render_mode="human")

model = PPO("MlpPolicy", env, verbose=1)
model.learn(total_timesteps=10000)

env.close()
```

### Option 2 : Test interactif avec ralentissement

**Fichier** : `scripts/play_game.py`

```python
import gymnasium as gym
from stable_baselines3 import PPO
import time

env = gym.make("CartPole-v1", render_mode="human")
model = PPO.load("models/ppo_cartpole")

obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated
    
    time.sleep(0.05)  # Ralentir pour mieux voir

env.close()
```

### Option 3 : Enregistrer une vidÃ©o

**Fichier** : `scripts/record_video.py`

```python
import gymnasium as gym
from gymnasium.wrappers import RecordVideo
from stable_baselines3 import PPO

env = gym.make("CartPole-v1")
env = RecordVideo(env, video_folder="videos/")

model = PPO.load("models/ppo_cartpole")

obs, _ = env.reset()
done = False

while not done:
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, terminated, truncated, _ = env.step(action)
    done = terminated or truncated

env.close()
print("âœ… VidÃ©o enregistrÃ©e dans 'videos/'")
```

### Option 4 : Tester sur plusieurs environnements diffÃ©rents

**Autres environnements simples Ã  tester** :

```python
# Essayer LunarLander (alunissage)
env = gym.make("LunarLander-v2", render_mode="human")

# Ou MountainCar (voiture en montagne)
env = gym.make("MountainCar-v0", render_mode="human")

# Ou Pendulum (pendule inversÃ© - continu)
env = gym.make("Pendulum-v1", render_mode="human")
```

---

## ğŸ“Š RÃ©sumÃ© des Ã©tapes d'exÃ©cution

```
1. âœ… Installer Python et dÃ©pendances (5 min)
   â†’ pip install stable-baselines3 gymnasium pygame numpy matplotlib

2. âœ… EntraÃ®ner PPO (2-3 min)
   â†’ python scripts/train_ppo.py

3. âœ… EntraÃ®ner DQN (2-3 min)
   â†’ python scripts/train_dqn.py

4. âœ… EntraÃ®ner SAC (2-3 min)
   â†’ python scripts/train_sac.py

5. âœ… Tester les agents (1 min)
   â†’ python scripts/test_agent.py

6. âœ… Comparer les rÃ©sultats (1-2 min)
   â†’ python scripts/benchmark_algos.py
   â†’ GÃ©nÃ©rera "results/comparaison_algos.png"

â±ï¸ Temps total : 15-20 minutes
```

---

## ğŸ” InterprÃ©tation des rÃ©sultats

### CartPole-v1
- **Objectif** : Garder un bÃ¢ton en Ã©quilibre sur un chariot mobile
- **Score maximal** : 500 (rÃ©ussite complÃ¨te)
- **Score acceptable** : > 400

### Comment interprÃ©ter les courbes :
1. **Moyenne** : Performance globale (plus haut = mieux)
2. **Ã‰cart-type** : StabilitÃ© (plus bas = plus stable)
3. **Min/Max** : Consistency (Min proche de la Moyenne = bon)

### RÃ©sultats typiques :
- **PPO** : Stable, performant (environ 450-500)
- **DQN** : Peut Ãªtre instable avec peu de donnÃ©es
- **SAC** : Bon Ã©quilibre, mais peut converger moins vite

---

## ğŸš€ Extensions possibles

1. **Changez d'environnement** : LunarLander, MountainCar, etc.
2. **HyperparamÃ¨tres** : Ajustez `learning_rate`, `n_steps`, etc.
3. **EntraÃ®nement plus long** : Augmentez `total_timesteps`
4. **Atari games** : Installez `stable-baselines3[atari]` pour des jeux plus complexes
5. **Analyse** : CrÃ©ez d'autres graphiques (courbes d'apprentissage, etc.)

---

## âš ï¸ DÃ©pannage

| ProblÃ¨me | Solution |
|----------|----------|
| ImportError pour gymnasium | `pip install gymnasium` |
| ImportError pour stable-baselines3 | `pip install stable-baselines3` |
| La fenÃªtre de jeu ne s'affiche pas | VÃ©rifiez que pygame est installÃ© : `pip install pygame` |
| EntraÃ®nement trÃ¨s lent | RÃ©duisez `total_timesteps` pour les tests rapides |
| Erreur GPU | Remplacez `device="cuda"` par `device="cpu"` |

---

## ğŸ“ Fichiers de rÃ©fÃ©rence

- [Stable-Baselines3 Docs](https://stable-baselines3.readthedocs.io/)
- [Gymnasium Documentation](https://gymnasium.farama.org/)
- [PPO Paper](https://arxiv.org/abs/1707.06347)
- [DQN Paper](https://arxiv.org/abs/1312.5602)
- [SAC Paper](https://arxiv.org/abs/1801.01290)

---

---

## ğŸ BONUS : EntraÃ®ner les agents sur Snake !

Nous avons crÃ©Ã© un **environnement Snake personnalisÃ© avec Pygame** pour un visuel vraiment beau ! ğŸ¨

### Ã‰tape 0 : Structure crÃ©Ã©e

```
GroupeRL/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ snake_env.py          â† Environnement Snake personnalisÃ©
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ppo_snake.py
â”‚   â”œâ”€â”€ train_dqn_snake.py
â”‚   â”œâ”€â”€ train_sac_snake.py
â”‚   â”œâ”€â”€ test_snake.py         â† Voir les serpents jouer
â”‚   â””â”€â”€ benchmark_snake.py    â† Comparer les algos sur Snake
â”œâ”€â”€ models/
â”‚   â””â”€â”€ ppo/dqn/sac_snake.zip
â””â”€â”€ results/
    â””â”€â”€ comparaison_snake.png
```

### Ã‰tape 1 : EntraÃ®ner les agents sur Snake

#### 1.1 EntraÃ®ner PPO (3-5 min)
```bash
python scripts/train_ppo_snake.py
```

#### 1.2 EntraÃ®ner DQN (3-5 min)
```bash
python scripts/train_dqn_snake.py
```

#### 1.3 EntraÃ®ner SAC (3-5 min)
```bash
python scripts/train_sac_snake.py
```

**Temps total** : ~15 minutes

### Ã‰tape 2 : Voir les serpents jouer avec Pygame ğŸ

```bash
python scripts/test_snake.py
```

**Vous verrez** :
- âœ… Une fenÃªtre avec une grille 10x10
- ğŸ Un serpent vert qui se dÃ©place
- ğŸ Une pomme rouge Ã  manger
- ğŸ“Š Le score (pommes mangÃ©es) en haut
- ğŸ® 3 Ã©pisodes par algorithme

**Visuel** :
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ğŸğŸğŸ             â”‚
â”‚       ğŸ            â”‚  â† Actions : 0=Haut, 1=Droite
â”‚                     â”‚    2=Bas, 3=Gauche
â”‚                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
Score: 5  Length: 4  Steps: 127
```

### Ã‰tape 3 : Comparer les performances (2 min)

```bash
python scripts/benchmark_snake.py
```

**GÃ©nÃ¨re** :
- 6 graphiques comparant PPO vs DQN vs SAC
- Tableau rÃ©capitulatif des rÃ©sultats
- Quel algo est le meilleur pour Snake ? ğŸ†
- Fichier : `results/comparaison_snake.png`

---

### ğŸ® Comment fonctionne Snake-v0 ?

**Objectif** : Manger le plus de pommes possible

**Actions** : 4 directions
- 0 = Haut â¬†ï¸
- 1 = Droite â¡ï¸
- 2 = Bas â¬‡ï¸
- 3 = Gauche â¬…ï¸

**Observation** : 6 variables (normalisÃ©es entre 0 et 1)
1. Position X de la tÃªte
2. Position Y de la tÃªte
3. Position X de la pomme
4. Position Y de la pomme
5. Direction courante (0-3)
6. Longueur du serpent (0-1)

**RÃ©compense** :
- +10 : Manger une pomme ğŸ
- +0.1 : Chaque step (encourager le mouvement)
- -10 : Collision avec mur ou corps

**Fin du jeu** :
- Collision avec mur ou le corps du serpent
- Ou 500 steps (dÃ©passement du temps limite)

---

### ğŸ“Š RÃ©sumÃ© Snake vs CartPole

| Aspect | CartPole | Snake |
|--------|----------|-------|
| **ComplexitÃ©** | TrÃ¨s simple | Moyenne |
| **Environnement** | Gymnasium standard | PersonnalisÃ© avec Pygame |
| **Visuel** | Basique (texte) | Beau (Pygame) |
| **Actions** | 2 (gauche/droite) | 4 (4 directions) |
| **Observations** | 4 variables | 6 variables |
| **Score max** | 500 | IllimitÃ© |
| **EntraÃ®nement** | 50k steps | 100k steps |
| **Temps** | 2-3 min | 3-5 min |

---

### ğŸš€ Commandes rapides Snake

```powershell
# EntraÃ®ner les 3 algos (15 min total)
python scripts/train_ppo_snake.py
python scripts/train_dqn_snake.py
python scripts/train_sac_snake.py

# Voir les agents jouer
python scripts/test_snake.py

# Comparer les rÃ©sultats
python scripts/benchmark_snake.py
```

---

### ğŸ’¡ Remarques importantes

1. **Pygame est nÃ©cessaire** : InstallÃ© avec `pip install pygame` au dÃ©but âœ…

2. **EntraÃ®nement plus long** : Snake est plus complexe que CartPole
   - 100k steps vs 50k pour CartPole
   - Mais toujours rapide (~3-5 min par algo)

3. **RÃ©sultats prÃ©visibles** :
   - PPO : Bon et stable âœ…
   - DQN : Peut Ãªtre moins stable sur Snake
   - SAC : AdaptÃ© Ã  continuo, mais fonctionne aussi ici

4. **AmÃ©liorations possibles** :
   - Augmenter la grille (15x15, 20x20)
   - Ajouter des obstacles
   - Changer les rÃ©compenses
   - EntraÃ®ner plus longtemps

---

âœ¨ **Bon apprentissage avec Snake !** ğŸğŸ
