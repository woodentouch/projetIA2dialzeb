"""
Entra√Ænement d'un agent PPO sur CartPole-v1
"""

import gymnasium as gym
from stable_baselines3 import PPO
import os

# Cr√©er le dossier models s'il n'existe pas
os.makedirs("models", exist_ok=True)

print("=" * 60)
print("üöÄ Entra√Ænement PPO sur CartPole-v1")
print("=" * 60)

# Cr√©er l'environnement
env = gym.make("CartPole-v1")
print(f"‚úÖ Environnement cr√©√© : CartPole-v1")
print(f"   - Espace d'observation : {env.observation_space}")
print(f"   - Espace d'action : {env.action_space}")

# Cr√©er le mod√®le PPO avec les hyperparam√®tres
model = PPO(
    "MlpPolicy",
    env,
    n_steps=2048,           # Nombre de steps avant mise √† jour
    batch_size=64,          # Taille du batch
    n_epochs=10,            # Nombre d'epochs d'optimisation
    learning_rate=3e-4,     # Taux d'apprentissage
    verbose=1,              # Afficher les logs
    device="cpu"            # "cuda" si GPU disponible
)

print(f"\n‚úÖ Mod√®le PPO cr√©√© avec les hyperparam√®tres")
print(f"   - Learning rate : 3e-4")
print(f"   - N steps : 2048")
print(f"   - Batch size : 64")
print(f"   - N epochs : 10")

# Entra√Æner le mod√®le
print(f"\n‚è≥ Entra√Ænement en cours... (50,000 timesteps)")
print(f"   Cela devrait prendre environ 2-3 minutes...")
print("-" * 60)

model.learn(total_timesteps=50000)

# Sauvegarder le mod√®le
model.save("models/ppo_cartpole")
print("-" * 60)
print(f"\n‚úÖ Entra√Ænement PPO termin√© avec succ√®s !")
print(f"   Mod√®le sauvegard√© : models/ppo_cartpole.zip")

env.close()
print("‚úÖ Environnement ferm√©")
print("=" * 60)
